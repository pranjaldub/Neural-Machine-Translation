{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":132,"outputs":[{"output_type":"stream","text":"/kaggle/input/german-data/deu.txt\n/kaggle/input/data-files/glove.6B.100d.txt\n/kaggle/input/data-files/hin.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import print_function,division\nfrom builtins import range,input","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys,os,pandas as pd,numpy as np,matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input,LSTM,Dense,Embedding,Bidirectional,RepeatVector,Concatenate,\\\n    Activation,Dot,Lambda\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras.backend as K\nfrom keras.activations import softmax\nfrom keras.optimizers import RMSprop,Adam,SGD\nfrom keras.models import load_model\nimport keras","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying softmax over time axis\n#the dimension is N*T*D, so we will use axis 1 as axis 1 is time axis\ndef softmax_over_time(x):\n    #assert(k.ndim(x)>2)\n    return softmax(x,axis =1)\ndef selu(x):\n    return keras.activations.selu(x)\n    ","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#config parameters\nLATENT_DIM_DECODER = 400\nBATCH_SIZE =64\nEPOCHS = 150\nLATENT_DIM = 400\nNUM_SAMPLES = 10000\nMAX_SEQUENCE_LEN = 100\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where we will store the data\ninput_texts = [] # sentence in original language\ntarget_texts = [] # sentence in target language\ntarget_texts_inputs = [] # sentence in target language offset by 1\n\n\n# load in the data\n# download the data at: http://www.manythings.org/anki/\nt = 0\nfor line in open('/kaggle/input/german-data/deu.txt'):\n  # only keep a limited number of samples\n  t += 1\n  if t > NUM_SAMPLES:\n    break\n\n  # input and target are separated by tab\n  if '\\t' not in line:\n    continue\n\n  # split up the input and translation\n  input_text, translation, *rest = line.rstrip().split('\\t')\n\n  # make the target input and output\n  # recall we'll be using teacher forcing\n  target_text = translation + ' <eos>'\n  target_text_input = '<sos> ' + translation\n\n  input_texts.append(input_text)\n  target_texts.append(target_text)\n  target_texts_inputs.append(target_text_input)\nprint(\"num samples:\", len(input_texts))\n","execution_count":137,"outputs":[{"output_type":"stream","text":"num samples: 10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize the inputs\ntokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_inputs.fit_on_texts(input_texts)\ninput_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n\n# get the word to index mapping for input language\nword2idx_inputs = tokenizer_inputs.word_index\nprint('Found %s unique input tokens.' % len(word2idx_inputs))\n\n# determine maximum length input sequence\nmax_len_input = max(len(s) for s in input_sequences)\n\n# tokenize the outputs\n# don't filter out special characters\n# otherwise <sos> and <eos> won't appear\ntokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\ntokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\ntarget_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\ntarget_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n\n# get the word to index mapping for output language\nword2idx_outputs = tokenizer_outputs.word_index\nprint('Found %s unique output tokens.' % len(word2idx_outputs))\n\n# store number of output words for later\n# remember to add 1 since indexing starts at 1\nnum_words_output = len(word2idx_outputs) + 1\n\n# determine maximum length output sequence\nmax_len_target = max(len(s) for s in target_sequences)\n","execution_count":138,"outputs":[{"output_type":"stream","text":"Found 2240 unique input tokens.\nFound 4979 unique output tokens.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PADDING \n\n# pad the sequences\nencoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\nprint(\"encoder_data.shape:\", encoder_inputs.shape)\nprint(\"encoder_data[0]:\", encoder_inputs[0])\n\ndecoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\nprint(\"decoder_data[0]:\", decoder_inputs[0])\nprint(\"decoder_data.shape:\", decoder_inputs.shape)\n\ndecoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n","execution_count":139,"outputs":[{"output_type":"stream","text":"encoder_data.shape: (10000, 5)\nencoder_data[0]: [ 0  0  0  0 13]\ndecoder_data[0]: [   2 1540    0    0    0    0    0    0    0    0]\ndecoder_data.shape: (10000, 10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LOADING PRETRAINED WORD VECTORS\n# store all the pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('/kaggle/input/data-files/glove.6B.100d.txt')) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","execution_count":140,"outputs":[{"output_type":"stream","text":"Loading word vectors...\nFound 400000 word vectors.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EMBEDDING MATRIX\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx_inputs.items():\n  if i < MAX_NUM_WORDS:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector","execution_count":141,"outputs":[{"output_type":"stream","text":"Filling pre-trained embeddings...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create embedding layer\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=max_len_input,\n  trainable=False\n)\n","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create targets, since we cannot use sparse\n# categorical cross entropy when we have sequences\ndecoder_targets_one_hot = np.zeros(\n  (\n    len(input_texts),\n    max_len_target,\n    num_words_output\n  ),\n  dtype='float32'\n)\n\n# assign the values\nfor i, d in enumerate(decoder_targets):\n  for t, word in enumerate(d):\n    if word > 0:\n      decoder_targets_one_hot[i, t, word] = 1\n","execution_count":143,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model\n\n# Set up the encoder - simple!\nencoder_inputs_placeholder = Input(shape=(max_len_input,))\nx = embedding_layer(encoder_inputs_placeholder)\nencoder = Bidirectional(LSTM(\n  LATENT_DIM,\n  return_sequences=True,\n  recurrent_dropout = 0.35 # dropout not available on gpu\n))\nencoder_outputs = encoder(x)\n\n\n# Set up the decoder - not so simple\ndecoder_inputs_placeholder = Input(shape=(max_len_target,))\n\n# this word embedding will not use pre-trained vectors\n# although you could\ndecoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\ndecoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n\n\n\n\n######### Attention #########\n# Attention layers need to be global because\n# they will be repeated Ty times at the decoder\nattn_repeat_layer = RepeatVector(max_len_input)\nattn_concat_layer = Concatenate(axis=-1)\nattn_dense1 = Dense(10, activation='tanh')\nattn_dense2 = Dense(1, activation=softmax_over_time)\nattn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n\ndef one_step_attention(h, st_1):\n  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n \n  # copy s(t-1) Tx times\n  # now shape = (Tx, LATENT_DIM_DECODER)\n  st_1 = attn_repeat_layer(st_1)\n\n  # Concatenate all h(t)'s with s(t-1)\n  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n  x = attn_concat_layer([h, st_1])\n\n  # Neural net first layer\n  x = attn_dense1(x)\n\n  # Neural net second layer with special softmax over time\n  alphas = attn_dense2(x)\n\n  # \"Dot\" the alphas and the h's\n  # Remember a.dot(b) = sum over a[t] * b[t]\n  context = attn_dot([alphas, h])\n\n  return context\n\n\n# define the rest of the decoder (after attention)\ndecoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True , recurrent_dropout = 0.35)\ndecoder_dense = Dense(num_words_output, activation='softmax')\n\ninitial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\ninitial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\ncontext_last_word_concat_layer = Concatenate(axis=2)\n\n\n# Unlike previous seq2seq, we cannot get the output\n# all in one step\n# Instead we need to do Ty steps\n# And in each of those steps, we need to consider\n# all Tx h's\n\n# s, c will be re-assigned in each iteration of the loop\ns = initial_s\nc = initial_c\n","execution_count":144,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collect outputs in a list at first\noutputs = []\nfor t in range(max_len_target): # Ty times\n  # get the context using attention\n  context = one_step_attention(encoder_outputs, s)\n\n  # we need a different layer for each time step\n  selector = Lambda(lambda x: x[:, t:t+1])\n  xt = selector(decoder_inputs_x)\n  \n  # combine \n  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n\n  # pass the combined [context, last word] into the LSTM\n  # along with [s, c]\n  # get the new [s, c] and output\n  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n\n  # final dense layer to get next word prediction\n  decoder_outputs = decoder_dense(o)\n  outputs.append(decoder_outputs)\n\n\n# 'outputs' is now a list of length Ty\n# each element is of shape (batch size, output vocab size)\n# therefore if we simply stack all the outputs into 1 tensor\n# it would be of shape T x N x D\n# we would like it to be of shape N x T x D\n\n","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stack_and_transpose(x):\n  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n  return x\n\n# make it a layer\nstacker = Lambda(stack_and_transpose)\noutputs = stacker(outputs)\n","execution_count":146,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel = Model(\n  inputs=[\n    encoder_inputs_placeholder,\n    decoder_inputs_placeholder,\n    initial_s, \n    initial_c,\n  ],\n  outputs=outputs\n)\n","execution_count":147,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n  # both are of shape N x T x K\n  mask = K.cast(y_true > 0, dtype='float32')\n  out = mask * y_true * K.log(y_pred)\n  return -K.sum(out) / K.sum(mask)\ndef acc(y_true, y_pred):\n  # both are of shape N x T x K\n  targ = K.argmax(y_true, axis=-1)\n  pred = K.argmax(y_pred, axis=-1)\n  correct = K.cast(K.equal(targ, pred), dtype='float32')\n\n  # 0 is padding, don't include those\n  mask = K.cast(K.greater(targ, 0), dtype='float32')\n  n_correct = K.sum(mask * correct)\n  n_total = K.sum(mask)\n  return n_correct / n_total\ndef ppl_e(y_true, y_pred):\n    return K.exp(K.mean(K.categorical_crossentropy(y_true, y_pred)))\n\nopt = Adam(lr=0.0001)\n#COMPILING THE MODEL\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":148,"outputs":[{"output_type":"stream","text":"Model: \"model_7\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_13 (InputLayer)           (None, 5)            0                                            \n__________________________________________________________________________________________________\nembedding_13 (Embedding)        (None, 5, 100)       224100      input_13[0][0]                   \n__________________________________________________________________________________________________\ns0 (InputLayer)                 (None, 400)          0                                            \n__________________________________________________________________________________________________\nbidirectional_7 (Bidirectional) (None, 5, 800)       1603200     embedding_13[0][0]               \n__________________________________________________________________________________________________\nrepeat_vector_7 (RepeatVector)  (None, 5, 400)       0           s0[0][0]                         \n                                                                 lstm_14[0][1]                    \n                                                                 lstm_14[1][1]                    \n                                                                 lstm_14[2][1]                    \n                                                                 lstm_14[3][1]                    \n                                                                 lstm_14[4][1]                    \n                                                                 lstm_14[5][1]                    \n                                                                 lstm_14[6][1]                    \n                                                                 lstm_14[7][1]                    \n                                                                 lstm_14[8][1]                    \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 5, 1200)      0           bidirectional_7[0][0]            \n                                                                 repeat_vector_7[0][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[1][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[2][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[3][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[4][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[5][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[6][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[7][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[8][0]            \n                                                                 bidirectional_7[0][0]            \n                                                                 repeat_vector_7[9][0]            \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 5, 10)        12010       concatenate_13[0][0]             \n                                                                 concatenate_13[1][0]             \n                                                                 concatenate_13[2][0]             \n                                                                 concatenate_13[3][0]             \n                                                                 concatenate_13[4][0]             \n                                                                 concatenate_13[5][0]             \n                                                                 concatenate_13[6][0]             \n                                                                 concatenate_13[7][0]             \n                                                                 concatenate_13[8][0]             \n                                                                 concatenate_13[9][0]             \n__________________________________________________________________________________________________\ninput_14 (InputLayer)           (None, 10)           0                                            \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 5, 1)         11          dense_19[0][0]                   \n                                                                 dense_19[1][0]                   \n                                                                 dense_19[2][0]                   \n                                                                 dense_19[3][0]                   \n                                                                 dense_19[4][0]                   \n                                                                 dense_19[5][0]                   \n                                                                 dense_19[6][0]                   \n                                                                 dense_19[7][0]                   \n                                                                 dense_19[8][0]                   \n                                                                 dense_19[9][0]                   \n__________________________________________________________________________________________________\nembedding_14 (Embedding)        (None, 10, 100)      498000      input_14[0][0]                   \n__________________________________________________________________________________________________\ndot_7 (Dot)                     (None, 1, 800)       0           dense_20[0][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[1][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[2][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[3][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[4][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[5][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[6][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[7][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[8][0]                   \n                                                                 bidirectional_7[0][0]            \n                                                                 dense_20[9][0]                   \n                                                                 bidirectional_7[0][0]            \n__________________________________________________________________________________________________\nlambda_67 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 1, 900)       0           dot_7[0][0]                      \n                                                                 lambda_67[0][0]                  \n                                                                 dot_7[1][0]                      \n                                                                 lambda_68[0][0]                  \n                                                                 dot_7[2][0]                      \n                                                                 lambda_69[0][0]                  \n                                                                 dot_7[3][0]                      \n                                                                 lambda_70[0][0]                  \n                                                                 dot_7[4][0]                      \n                                                                 lambda_71[0][0]                  \n                                                                 dot_7[5][0]                      \n                                                                 lambda_72[0][0]                  \n                                                                 dot_7[6][0]                      \n                                                                 lambda_73[0][0]                  \n                                                                 dot_7[7][0]                      \n                                                                 lambda_74[0][0]                  \n                                                                 dot_7[8][0]                      \n                                                                 lambda_75[0][0]                  \n                                                                 dot_7[9][0]                      \n                                                                 lambda_76[0][0]                  \n__________________________________________________________________________________________________\nc0 (InputLayer)                 (None, 400)          0                                            \n__________________________________________________________________________________________________\nlstm_14 (LSTM)                  [(None, 400), (None, 2081600     concatenate_14[0][0]             \n                                                                 s0[0][0]                         \n                                                                 c0[0][0]                         \n                                                                 concatenate_14[1][0]             \n                                                                 lstm_14[0][1]                    \n                                                                 lstm_14[0][2]                    \n                                                                 concatenate_14[2][0]             \n                                                                 lstm_14[1][1]                    \n                                                                 lstm_14[1][2]                    \n                                                                 concatenate_14[3][0]             \n                                                                 lstm_14[2][1]                    \n                                                                 lstm_14[2][2]                    \n                                                                 concatenate_14[4][0]             \n                                                                 lstm_14[3][1]                    \n                                                                 lstm_14[3][2]                    \n                                                                 concatenate_14[5][0]             \n                                                                 lstm_14[4][1]                    \n                                                                 lstm_14[4][2]                    \n                                                                 concatenate_14[6][0]             \n                                                                 lstm_14[5][1]                    \n                                                                 lstm_14[5][2]                    \n                                                                 concatenate_14[7][0]             \n                                                                 lstm_14[6][1]                    \n                                                                 lstm_14[6][2]                    \n                                                                 concatenate_14[8][0]             \n                                                                 lstm_14[7][1]                    \n                                                                 lstm_14[7][2]                    \n                                                                 concatenate_14[9][0]             \n                                                                 lstm_14[8][1]                    \n                                                                 lstm_14[8][2]                    \n__________________________________________________________________________________________________\nlambda_68 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_69 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_70 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_71 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_72 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_73 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_74 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_75 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_76 (Lambda)              (None, 1, 100)       0           embedding_14[0][0]               \n__________________________________________________________________________________________________\ndense_21 (Dense)                (None, 4980)         1996980     lstm_14[0][0]                    \n                                                                 lstm_14[1][0]                    \n                                                                 lstm_14[2][0]                    \n                                                                 lstm_14[3][0]                    \n                                                                 lstm_14[4][0]                    \n                                                                 lstm_14[5][0]                    \n                                                                 lstm_14[6][0]                    \n                                                                 lstm_14[7][0]                    \n                                                                 lstm_14[8][0]                    \n                                                                 lstm_14[9][0]                    \n__________________________________________________________________________________________________\nlambda_77 (Lambda)              (None, 10, 4980)     0           dense_21[0][0]                   \n                                                                 dense_21[1][0]                   \n                                                                 dense_21[2][0]                   \n                                                                 dense_21[3][0]                   \n                                                                 dense_21[4][0]                   \n                                                                 dense_21[5][0]                   \n                                                                 dense_21[6][0]                   \n                                                                 dense_21[7][0]                   \n                                                                 dense_21[8][0]                   \n                                                                 dense_21[9][0]                   \n==================================================================================================\nTotal params: 6,415,901\nTrainable params: 6,191,801\nNon-trainable params: 224,100\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRAIN THE MODEL\n#implementing callbacks\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\ncheckpoint = ModelCheckpoint(\"englisg_german.h5\",\n                            monitor=\"val_loss\",\n                            mode=\"min\",\n                            save_best_only=True,\n                            verbose=1)\nearly_stopping = EarlyStopping(monitor=\"val_loss\",\n                              min_delta=0,\n                              patience=10,\n                              verbose=1,\n                              restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n                             factor=0.1,\n                             patience=3,\n                             verbose=1,\n                             min_delta=0.0001)\n#putting callbacks in callbacks list\ncallbacks = [checkpoint,early_stopping,reduce_lr]\nz = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\nr = model.fit(\n  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=0.2 , callbacks= callbacks\n)","execution_count":149,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 8000 samples, validate on 2000 samples\nEpoch 1/150\n8000/8000 [==============================] - 18s 2ms/step - loss: 2.8606 - accuracy: 0.0975 - val_loss: 2.6060 - val_accuracy: 0.1007\n\nEpoch 00001: val_loss improved from inf to 2.60604, saving model to englisg_german.h5\nEpoch 2/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 2.2323 - accuracy: 0.1251 - val_loss: 2.4804 - val_accuracy: 0.1044\n\nEpoch 00002: val_loss improved from 2.60604 to 2.48035, saving model to englisg_german.h5\nEpoch 3/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 2.1274 - accuracy: 0.1290 - val_loss: 2.4181 - val_accuracy: 0.1225\n\nEpoch 00003: val_loss improved from 2.48035 to 2.41807, saving model to englisg_german.h5\nEpoch 4/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 2.0764 - accuracy: 0.1348 - val_loss: 2.3876 - val_accuracy: 0.1248\n\nEpoch 00004: val_loss improved from 2.41807 to 2.38761, saving model to englisg_german.h5\nEpoch 5/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 2.0415 - accuracy: 0.1355 - val_loss: 2.3658 - val_accuracy: 0.1249\n\nEpoch 00005: val_loss improved from 2.38761 to 2.36584, saving model to englisg_german.h5\nEpoch 6/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 2.0106 - accuracy: 0.1355 - val_loss: 2.3404 - val_accuracy: 0.1249\n\nEpoch 00006: val_loss improved from 2.36584 to 2.34035, saving model to englisg_german.h5\nEpoch 7/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.9788 - accuracy: 0.1356 - val_loss: 2.3130 - val_accuracy: 0.1250\n\nEpoch 00007: val_loss improved from 2.34035 to 2.31304, saving model to englisg_german.h5\nEpoch 8/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.9423 - accuracy: 0.1360 - val_loss: 2.2818 - val_accuracy: 0.1259\n\nEpoch 00008: val_loss improved from 2.31304 to 2.28183, saving model to englisg_german.h5\nEpoch 9/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.9046 - accuracy: 0.1378 - val_loss: 2.2546 - val_accuracy: 0.1270\n\nEpoch 00009: val_loss improved from 2.28183 to 2.25455, saving model to englisg_german.h5\nEpoch 10/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.8639 - accuracy: 0.1410 - val_loss: 2.2125 - val_accuracy: 0.1279\n\nEpoch 00010: val_loss improved from 2.25455 to 2.21251, saving model to englisg_german.h5\nEpoch 11/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.8254 - accuracy: 0.1449 - val_loss: 2.1783 - val_accuracy: 0.1308\n\nEpoch 00011: val_loss improved from 2.21251 to 2.17826, saving model to englisg_german.h5\nEpoch 12/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.7893 - accuracy: 0.1495 - val_loss: 2.1560 - val_accuracy: 0.1576\n\nEpoch 00012: val_loss improved from 2.17826 to 2.15601, saving model to englisg_german.h5\nEpoch 13/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.7560 - accuracy: 0.1577 - val_loss: 2.1260 - val_accuracy: 0.1648\n\nEpoch 00013: val_loss improved from 2.15601 to 2.12596, saving model to englisg_german.h5\nEpoch 14/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.7248 - accuracy: 0.1603 - val_loss: 2.1089 - val_accuracy: 0.1654\n\nEpoch 00014: val_loss improved from 2.12596 to 2.10892, saving model to englisg_german.h5\nEpoch 15/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.6943 - accuracy: 0.1615 - val_loss: 2.0854 - val_accuracy: 0.1657\n\nEpoch 00015: val_loss improved from 2.10892 to 2.08540, saving model to englisg_german.h5\nEpoch 16/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.6667 - accuracy: 0.1631 - val_loss: 2.0690 - val_accuracy: 0.1671\n\nEpoch 00016: val_loss improved from 2.08540 to 2.06901, saving model to englisg_german.h5\nEpoch 17/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.6391 - accuracy: 0.1652 - val_loss: 2.0474 - val_accuracy: 0.1708\n\nEpoch 00017: val_loss improved from 2.06901 to 2.04740, saving model to englisg_german.h5\nEpoch 18/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.6116 - accuracy: 0.1675 - val_loss: 2.0281 - val_accuracy: 0.1726\n\nEpoch 00018: val_loss improved from 2.04740 to 2.02807, saving model to englisg_german.h5\nEpoch 19/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.5853 - accuracy: 0.1696 - val_loss: 2.0101 - val_accuracy: 0.1750\n\nEpoch 00019: val_loss improved from 2.02807 to 2.01006, saving model to englisg_german.h5\nEpoch 20/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.5588 - accuracy: 0.1726 - val_loss: 1.9962 - val_accuracy: 0.1777\n\nEpoch 00020: val_loss improved from 2.01006 to 1.99624, saving model to englisg_german.h5\nEpoch 21/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.5336 - accuracy: 0.1750 - val_loss: 1.9806 - val_accuracy: 0.1782\n\nEpoch 00021: val_loss improved from 1.99624 to 1.98059, saving model to englisg_german.h5\nEpoch 22/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.5087 - accuracy: 0.1779 - val_loss: 1.9658 - val_accuracy: 0.1812\n\nEpoch 00022: val_loss improved from 1.98059 to 1.96583, saving model to englisg_german.h5\nEpoch 23/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.4848 - accuracy: 0.1801 - val_loss: 1.9500 - val_accuracy: 0.1851\n\nEpoch 00023: val_loss improved from 1.96583 to 1.95004, saving model to englisg_german.h5\nEpoch 24/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.4613 - accuracy: 0.1832 - val_loss: 1.9390 - val_accuracy: 0.1866\n\nEpoch 00024: val_loss improved from 1.95004 to 1.93904, saving model to englisg_german.h5\nEpoch 25/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.4383 - accuracy: 0.1858 - val_loss: 1.9293 - val_accuracy: 0.1893\n\nEpoch 00025: val_loss improved from 1.93904 to 1.92934, saving model to englisg_german.h5\nEpoch 26/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.4153 - accuracy: 0.1885 - val_loss: 1.9180 - val_accuracy: 0.1914\n\nEpoch 00026: val_loss improved from 1.92934 to 1.91800, saving model to englisg_german.h5\nEpoch 27/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.3942 - accuracy: 0.1908 - val_loss: 1.9120 - val_accuracy: 0.1921\n\nEpoch 00027: val_loss improved from 1.91800 to 1.91201, saving model to englisg_german.h5\nEpoch 28/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.3722 - accuracy: 0.1937 - val_loss: 1.9027 - val_accuracy: 0.1929\n\nEpoch 00028: val_loss improved from 1.91201 to 1.90266, saving model to englisg_german.h5\nEpoch 29/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.3518 - accuracy: 0.1956 - val_loss: 1.8930 - val_accuracy: 0.1947\n\nEpoch 00029: val_loss improved from 1.90266 to 1.89305, saving model to englisg_german.h5\nEpoch 30/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.3306 - accuracy: 0.1979 - val_loss: 1.8839 - val_accuracy: 0.1958\n\nEpoch 00030: val_loss improved from 1.89305 to 1.88389, saving model to englisg_german.h5\nEpoch 31/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.3101 - accuracy: 0.1999 - val_loss: 1.8777 - val_accuracy: 0.1986\n\nEpoch 00031: val_loss improved from 1.88389 to 1.87765, saving model to englisg_german.h5\nEpoch 32/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.2901 - accuracy: 0.2020 - val_loss: 1.8672 - val_accuracy: 0.1996\n\nEpoch 00032: val_loss improved from 1.87765 to 1.86723, saving model to englisg_german.h5\nEpoch 33/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.2700 - accuracy: 0.2037 - val_loss: 1.8621 - val_accuracy: 0.2005\n\nEpoch 00033: val_loss improved from 1.86723 to 1.86207, saving model to englisg_german.h5\nEpoch 34/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.2508 - accuracy: 0.2055 - val_loss: 1.8573 - val_accuracy: 0.2029\n\nEpoch 00034: val_loss improved from 1.86207 to 1.85731, saving model to englisg_german.h5\nEpoch 35/150\n","name":"stdout"},{"output_type":"stream","text":"8000/8000 [==============================] - 9s 1ms/step - loss: 1.2314 - accuracy: 0.2081 - val_loss: 1.8499 - val_accuracy: 0.2028\n\nEpoch 00035: val_loss improved from 1.85731 to 1.84991, saving model to englisg_german.h5\nEpoch 36/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.2120 - accuracy: 0.2103 - val_loss: 1.8397 - val_accuracy: 0.2079\n\nEpoch 00036: val_loss improved from 1.84991 to 1.83970, saving model to englisg_german.h5\nEpoch 37/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.1932 - accuracy: 0.2121 - val_loss: 1.8359 - val_accuracy: 0.2074\n\nEpoch 00037: val_loss improved from 1.83970 to 1.83590, saving model to englisg_german.h5\nEpoch 38/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.1746 - accuracy: 0.2142 - val_loss: 1.8301 - val_accuracy: 0.2098\n\nEpoch 00038: val_loss improved from 1.83590 to 1.83009, saving model to englisg_german.h5\nEpoch 39/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.1559 - accuracy: 0.2165 - val_loss: 1.8251 - val_accuracy: 0.2122\n\nEpoch 00039: val_loss improved from 1.83009 to 1.82510, saving model to englisg_german.h5\nEpoch 40/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.1377 - accuracy: 0.2183 - val_loss: 1.8187 - val_accuracy: 0.2132\n\nEpoch 00040: val_loss improved from 1.82510 to 1.81868, saving model to englisg_german.h5\nEpoch 41/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.1201 - accuracy: 0.2205 - val_loss: 1.8099 - val_accuracy: 0.2154\n\nEpoch 00041: val_loss improved from 1.81868 to 1.80987, saving model to englisg_german.h5\nEpoch 42/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.1021 - accuracy: 0.2220 - val_loss: 1.8062 - val_accuracy: 0.2162\n\nEpoch 00042: val_loss improved from 1.80987 to 1.80620, saving model to englisg_german.h5\nEpoch 43/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.0858 - accuracy: 0.2237 - val_loss: 1.7989 - val_accuracy: 0.2163\n\nEpoch 00043: val_loss improved from 1.80620 to 1.79887, saving model to englisg_german.h5\nEpoch 44/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.0679 - accuracy: 0.2260 - val_loss: 1.7941 - val_accuracy: 0.2174\n\nEpoch 00044: val_loss improved from 1.79887 to 1.79409, saving model to englisg_german.h5\nEpoch 45/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 1.0512 - accuracy: 0.2277 - val_loss: 1.7899 - val_accuracy: 0.2176\n\nEpoch 00045: val_loss improved from 1.79409 to 1.78990, saving model to englisg_german.h5\nEpoch 46/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.0351 - accuracy: 0.2298 - val_loss: 1.7867 - val_accuracy: 0.2182\n\nEpoch 00046: val_loss improved from 1.78990 to 1.78673, saving model to englisg_german.h5\nEpoch 47/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.0191 - accuracy: 0.2316 - val_loss: 1.7817 - val_accuracy: 0.2195\n\nEpoch 00047: val_loss improved from 1.78673 to 1.78169, saving model to englisg_german.h5\nEpoch 48/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 1.0027 - accuracy: 0.2327 - val_loss: 1.7775 - val_accuracy: 0.2207\n\nEpoch 00048: val_loss improved from 1.78169 to 1.77755, saving model to englisg_german.h5\nEpoch 49/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.9876 - accuracy: 0.2341 - val_loss: 1.7721 - val_accuracy: 0.2221\n\nEpoch 00049: val_loss improved from 1.77755 to 1.77213, saving model to englisg_german.h5\nEpoch 50/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.9718 - accuracy: 0.2361 - val_loss: 1.7680 - val_accuracy: 0.2221\n\nEpoch 00050: val_loss improved from 1.77213 to 1.76797, saving model to englisg_german.h5\nEpoch 51/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.9567 - accuracy: 0.2374 - val_loss: 1.7632 - val_accuracy: 0.2229\n\nEpoch 00051: val_loss improved from 1.76797 to 1.76317, saving model to englisg_german.h5\nEpoch 52/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.9419 - accuracy: 0.2393 - val_loss: 1.7590 - val_accuracy: 0.2230\n\nEpoch 00052: val_loss improved from 1.76317 to 1.75903, saving model to englisg_german.h5\nEpoch 53/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.9271 - accuracy: 0.2410 - val_loss: 1.7545 - val_accuracy: 0.2240\n\nEpoch 00053: val_loss improved from 1.75903 to 1.75448, saving model to englisg_german.h5\nEpoch 54/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.9125 - accuracy: 0.2425 - val_loss: 1.7521 - val_accuracy: 0.2247\n\nEpoch 00054: val_loss improved from 1.75448 to 1.75211, saving model to englisg_german.h5\nEpoch 55/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8982 - accuracy: 0.2443 - val_loss: 1.7467 - val_accuracy: 0.2250\n\nEpoch 00055: val_loss improved from 1.75211 to 1.74668, saving model to englisg_german.h5\nEpoch 56/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8834 - accuracy: 0.2465 - val_loss: 1.7462 - val_accuracy: 0.2254\n\nEpoch 00056: val_loss improved from 1.74668 to 1.74618, saving model to englisg_german.h5\nEpoch 57/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.8701 - accuracy: 0.2480 - val_loss: 1.7414 - val_accuracy: 0.2265\n\nEpoch 00057: val_loss improved from 1.74618 to 1.74141, saving model to englisg_german.h5\nEpoch 58/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.8559 - accuracy: 0.2501 - val_loss: 1.7384 - val_accuracy: 0.2268\n\nEpoch 00058: val_loss improved from 1.74141 to 1.73836, saving model to englisg_german.h5\nEpoch 59/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8417 - accuracy: 0.2516 - val_loss: 1.7324 - val_accuracy: 0.2274\n\nEpoch 00059: val_loss improved from 1.73836 to 1.73243, saving model to englisg_german.h5\nEpoch 60/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8283 - accuracy: 0.2536 - val_loss: 1.7290 - val_accuracy: 0.2280\n\nEpoch 00060: val_loss improved from 1.73243 to 1.72897, saving model to englisg_german.h5\nEpoch 61/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8143 - accuracy: 0.2555 - val_loss: 1.7289 - val_accuracy: 0.2288\n\nEpoch 00061: val_loss improved from 1.72897 to 1.72886, saving model to englisg_german.h5\nEpoch 62/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.8013 - accuracy: 0.2576 - val_loss: 1.7234 - val_accuracy: 0.2298\n\nEpoch 00062: val_loss improved from 1.72886 to 1.72341, saving model to englisg_german.h5\nEpoch 63/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7882 - accuracy: 0.2594 - val_loss: 1.7230 - val_accuracy: 0.2300\n\nEpoch 00063: val_loss improved from 1.72341 to 1.72300, saving model to englisg_german.h5\nEpoch 64/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.7751 - accuracy: 0.2619 - val_loss: 1.7183 - val_accuracy: 0.2308\n\nEpoch 00064: val_loss improved from 1.72300 to 1.71829, saving model to englisg_german.h5\nEpoch 65/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7621 - accuracy: 0.2643 - val_loss: 1.7145 - val_accuracy: 0.2319\n\nEpoch 00065: val_loss improved from 1.71829 to 1.71452, saving model to englisg_german.h5\nEpoch 66/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7493 - accuracy: 0.2663 - val_loss: 1.7121 - val_accuracy: 0.2326\n\nEpoch 00066: val_loss improved from 1.71452 to 1.71210, saving model to englisg_german.h5\nEpoch 67/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7373 - accuracy: 0.2678 - val_loss: 1.7074 - val_accuracy: 0.2333\n\nEpoch 00067: val_loss improved from 1.71210 to 1.70740, saving model to englisg_german.h5\nEpoch 68/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7245 - accuracy: 0.2712 - val_loss: 1.7062 - val_accuracy: 0.2331\n\nEpoch 00068: val_loss improved from 1.70740 to 1.70625, saving model to englisg_german.h5\nEpoch 69/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7124 - accuracy: 0.2736 - val_loss: 1.7046 - val_accuracy: 0.2330\n","name":"stdout"},{"output_type":"stream","text":"\nEpoch 00069: val_loss improved from 1.70625 to 1.70458, saving model to englisg_german.h5\nEpoch 70/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.7013 - accuracy: 0.2750 - val_loss: 1.7025 - val_accuracy: 0.2340\n\nEpoch 00070: val_loss improved from 1.70458 to 1.70248, saving model to englisg_german.h5\nEpoch 71/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.6897 - accuracy: 0.2772 - val_loss: 1.6979 - val_accuracy: 0.2345\n\nEpoch 00071: val_loss improved from 1.70248 to 1.69787, saving model to englisg_german.h5\nEpoch 72/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6788 - accuracy: 0.2796 - val_loss: 1.6978 - val_accuracy: 0.2342\n\nEpoch 00072: val_loss improved from 1.69787 to 1.69776, saving model to englisg_german.h5\nEpoch 73/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6675 - accuracy: 0.2818 - val_loss: 1.6945 - val_accuracy: 0.2349\n\nEpoch 00073: val_loss improved from 1.69776 to 1.69448, saving model to englisg_german.h5\nEpoch 74/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6564 - accuracy: 0.2840 - val_loss: 1.6938 - val_accuracy: 0.2354\n\nEpoch 00074: val_loss improved from 1.69448 to 1.69378, saving model to englisg_german.h5\nEpoch 75/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6453 - accuracy: 0.2864 - val_loss: 1.6920 - val_accuracy: 0.2361\n\nEpoch 00075: val_loss improved from 1.69378 to 1.69202, saving model to englisg_german.h5\nEpoch 76/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.6349 - accuracy: 0.2884 - val_loss: 1.6891 - val_accuracy: 0.2362\n\nEpoch 00076: val_loss improved from 1.69202 to 1.68912, saving model to englisg_german.h5\nEpoch 77/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.6246 - accuracy: 0.2900 - val_loss: 1.6890 - val_accuracy: 0.2372\n\nEpoch 00077: val_loss improved from 1.68912 to 1.68905, saving model to englisg_german.h5\nEpoch 78/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6144 - accuracy: 0.2933 - val_loss: 1.6868 - val_accuracy: 0.2368\n\nEpoch 00078: val_loss improved from 1.68905 to 1.68677, saving model to englisg_german.h5\nEpoch 79/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.6046 - accuracy: 0.2946 - val_loss: 1.6821 - val_accuracy: 0.2378\n\nEpoch 00079: val_loss improved from 1.68677 to 1.68213, saving model to englisg_german.h5\nEpoch 80/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5943 - accuracy: 0.2971 - val_loss: 1.6812 - val_accuracy: 0.2383\n\nEpoch 00080: val_loss improved from 1.68213 to 1.68116, saving model to englisg_german.h5\nEpoch 81/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5844 - accuracy: 0.2989 - val_loss: 1.6797 - val_accuracy: 0.2392\n\nEpoch 00081: val_loss improved from 1.68116 to 1.67970, saving model to englisg_german.h5\nEpoch 82/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5752 - accuracy: 0.3010 - val_loss: 1.6768 - val_accuracy: 0.2392\n\nEpoch 00082: val_loss improved from 1.67970 to 1.67685, saving model to englisg_german.h5\nEpoch 83/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.5660 - accuracy: 0.3019 - val_loss: 1.6755 - val_accuracy: 0.2400\n\nEpoch 00083: val_loss improved from 1.67685 to 1.67550, saving model to englisg_german.h5\nEpoch 84/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5567 - accuracy: 0.3045 - val_loss: 1.6765 - val_accuracy: 0.2390\n\nEpoch 00084: val_loss did not improve from 1.67550\nEpoch 85/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5475 - accuracy: 0.3068 - val_loss: 1.6746 - val_accuracy: 0.2406\n\nEpoch 00085: val_loss improved from 1.67550 to 1.67460, saving model to englisg_german.h5\nEpoch 86/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5384 - accuracy: 0.3091 - val_loss: 1.6719 - val_accuracy: 0.2409\n\nEpoch 00086: val_loss improved from 1.67460 to 1.67190, saving model to englisg_german.h5\nEpoch 87/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5299 - accuracy: 0.3107 - val_loss: 1.6685 - val_accuracy: 0.2411\n\nEpoch 00087: val_loss improved from 1.67190 to 1.66851, saving model to englisg_german.h5\nEpoch 88/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5219 - accuracy: 0.3116 - val_loss: 1.6696 - val_accuracy: 0.2412\n\nEpoch 00088: val_loss did not improve from 1.66851\nEpoch 89/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.5139 - accuracy: 0.3131 - val_loss: 1.6684 - val_accuracy: 0.2421\n\nEpoch 00089: val_loss improved from 1.66851 to 1.66836, saving model to englisg_german.h5\nEpoch 90/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.5052 - accuracy: 0.3151 - val_loss: 1.6688 - val_accuracy: 0.2412\n\nEpoch 00090: val_loss did not improve from 1.66836\nEpoch 91/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4975 - accuracy: 0.3164 - val_loss: 1.6656 - val_accuracy: 0.2429\n\nEpoch 00091: val_loss improved from 1.66836 to 1.66565, saving model to englisg_german.h5\nEpoch 92/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4897 - accuracy: 0.3180 - val_loss: 1.6641 - val_accuracy: 0.2422\n\nEpoch 00092: val_loss improved from 1.66565 to 1.66414, saving model to englisg_german.h5\nEpoch 93/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4824 - accuracy: 0.3196 - val_loss: 1.6628 - val_accuracy: 0.2435\n\nEpoch 00093: val_loss improved from 1.66414 to 1.66276, saving model to englisg_german.h5\nEpoch 94/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4737 - accuracy: 0.3214 - val_loss: 1.6641 - val_accuracy: 0.2430\n\nEpoch 00094: val_loss did not improve from 1.66276\nEpoch 95/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4671 - accuracy: 0.3229 - val_loss: 1.6609 - val_accuracy: 0.2434\n\nEpoch 00095: val_loss improved from 1.66276 to 1.66092, saving model to englisg_german.h5\nEpoch 96/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4597 - accuracy: 0.3238 - val_loss: 1.6605 - val_accuracy: 0.2443\n\nEpoch 00096: val_loss improved from 1.66092 to 1.66046, saving model to englisg_german.h5\nEpoch 97/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4525 - accuracy: 0.3261 - val_loss: 1.6609 - val_accuracy: 0.2437\n\nEpoch 00097: val_loss did not improve from 1.66046\nEpoch 98/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4461 - accuracy: 0.3270 - val_loss: 1.6609 - val_accuracy: 0.2448\n\nEpoch 00098: val_loss did not improve from 1.66046\nEpoch 99/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4390 - accuracy: 0.3278 - val_loss: 1.6597 - val_accuracy: 0.2447\n\nEpoch 00099: val_loss improved from 1.66046 to 1.65970, saving model to englisg_german.h5\nEpoch 100/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4317 - accuracy: 0.3296 - val_loss: 1.6580 - val_accuracy: 0.2456\n\nEpoch 00100: val_loss improved from 1.65970 to 1.65796, saving model to englisg_german.h5\nEpoch 101/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4256 - accuracy: 0.3304 - val_loss: 1.6572 - val_accuracy: 0.2459\n\nEpoch 00101: val_loss improved from 1.65796 to 1.65722, saving model to englisg_german.h5\nEpoch 102/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4189 - accuracy: 0.3320 - val_loss: 1.6591 - val_accuracy: 0.2453\n\nEpoch 00102: val_loss did not improve from 1.65722\nEpoch 103/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4124 - accuracy: 0.3332 - val_loss: 1.6594 - val_accuracy: 0.2453\n\nEpoch 00103: val_loss did not improve from 1.65722\nEpoch 104/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4068 - accuracy: 0.3348 - val_loss: 1.6546 - val_accuracy: 0.2458\n\nEpoch 00104: val_loss improved from 1.65722 to 1.65463, saving model to englisg_german.h5\nEpoch 105/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4010 - accuracy: 0.3358 - val_loss: 1.6568 - val_accuracy: 0.2458\n","name":"stdout"},{"output_type":"stream","text":"\nEpoch 00105: val_loss did not improve from 1.65463\nEpoch 106/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3950 - accuracy: 0.3367 - val_loss: 1.6537 - val_accuracy: 0.2462\n\nEpoch 00106: val_loss improved from 1.65463 to 1.65370, saving model to englisg_german.h5\nEpoch 107/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3890 - accuracy: 0.3377 - val_loss: 1.6561 - val_accuracy: 0.2456\n\nEpoch 00107: val_loss did not improve from 1.65370\nEpoch 108/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3830 - accuracy: 0.3392 - val_loss: 1.6559 - val_accuracy: 0.2465\n\nEpoch 00108: val_loss did not improve from 1.65370\nEpoch 109/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3775 - accuracy: 0.3403 - val_loss: 1.6546 - val_accuracy: 0.2472\n\nEpoch 00109: val_loss did not improve from 1.65370\n\nEpoch 00109: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\nEpoch 110/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3651 - accuracy: 0.3450 - val_loss: 1.6543 - val_accuracy: 0.2472\n\nEpoch 00110: val_loss did not improve from 1.65370\nEpoch 111/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3641 - accuracy: 0.3446 - val_loss: 1.6535 - val_accuracy: 0.2471\n\nEpoch 00111: val_loss improved from 1.65370 to 1.65350, saving model to englisg_german.h5\nEpoch 112/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3632 - accuracy: 0.3456 - val_loss: 1.6539 - val_accuracy: 0.2469\n\nEpoch 00112: val_loss did not improve from 1.65350\nEpoch 113/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3625 - accuracy: 0.3454 - val_loss: 1.6536 - val_accuracy: 0.2470\n\nEpoch 00113: val_loss did not improve from 1.65350\nEpoch 114/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3620 - accuracy: 0.3455 - val_loss: 1.6538 - val_accuracy: 0.2468\n\nEpoch 00114: val_loss did not improve from 1.65350\n\nEpoch 00114: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\nEpoch 115/150\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.3609 - accuracy: 0.3462 - val_loss: 1.6539 - val_accuracy: 0.2470\n\nEpoch 00115: val_loss did not improve from 1.65350\nEpoch 116/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3609 - accuracy: 0.3464 - val_loss: 1.6539 - val_accuracy: 0.2471\n\nEpoch 00116: val_loss did not improve from 1.65350\nEpoch 117/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3604 - accuracy: 0.3462 - val_loss: 1.6538 - val_accuracy: 0.2470\n\nEpoch 00117: val_loss did not improve from 1.65350\n\nEpoch 00117: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\nEpoch 118/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3603 - accuracy: 0.3465 - val_loss: 1.6538 - val_accuracy: 0.2470\n\nEpoch 00118: val_loss did not improve from 1.65350\nEpoch 119/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3604 - accuracy: 0.3468 - val_loss: 1.6538 - val_accuracy: 0.2470\n\nEpoch 00119: val_loss did not improve from 1.65350\nEpoch 120/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3607 - accuracy: 0.3464 - val_loss: 1.6538 - val_accuracy: 0.2470\n\nEpoch 00120: val_loss did not improve from 1.65350\n\nEpoch 00120: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\nEpoch 121/150\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.3596 - accuracy: 0.3462 - val_loss: 1.6538 - val_accuracy: 0.2470\n\nEpoch 00121: val_loss did not improve from 1.65350\nRestoring model weights from the end of the best epoch\nEpoch 00121: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the accuracies\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\n\n# accuracies\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()","execution_count":150,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Make predictions #####\n# As with the poetry example, we need to create another model\n# that can take in the RNN state and previous word as input\n# and accept a T=1 sequence.\n\n# The encoder will be stand-alone\n# From this we will get our initial decoder hidden state\n# i.e. h(1), ..., h(Tx)\nencoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n\n# next we define a T=1 decoder model\nencoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\ndecoder_inputs_single = Input(shape=(1,))\ndecoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n\n# no need to loop over attention steps this time because there is only one step\ncontext = one_step_attention(encoder_outputs_as_input, initial_s)\n\n# combine context with last word\ndecoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n\n\n\n\n# lstm and final dense\no, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\ndecoder_outputs = decoder_dense(o)\n\n\n# note: we don't really need the final stack and tranpose\n# because there's only 1 output\n# it is already of size N x D\n# no need to make it 1 x N x D --> N x 1 x D","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model object\ndecoder_model = Model(\n  inputs=[\n    decoder_inputs_single,\n    encoder_outputs_as_input,\n    initial_s, \n    initial_c\n  ],\n  outputs=[decoder_outputs, s, c]\n)\n","execution_count":152,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# map indexes back into real words\n# so we can view the results\nidx2word_eng = {v:k for k, v in word2idx_inputs.items()}\nidx2word_trans = {v:k for k, v in word2idx_outputs.items()}","execution_count":153,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n  # Encode the input as state vectors.\n  enc_out = encoder_model.predict(input_seq)\n\n  # Generate empty target sequence of length 1.\n  target_seq = np.zeros((1, 1))\n  \n  # Populate the first character of target sequence with the start character.\n  # NOTE: tokenizer lower-cases all words\n  target_seq[0, 0] = word2idx_outputs['<sos>']\n\n  # if we get this we break\n  eos = word2idx_outputs['<eos>']\n\n\n  # [s, c] will be updated in each loop iteration\n  s = np.zeros((1, LATENT_DIM_DECODER))\n  c = np.zeros((1, LATENT_DIM_DECODER))\n\n\n  # Create the translation\n  output_sentence = []\n  for _ in range(max_len_target):\n    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n        \n\n    # Get next word\n    idx = np.argmax(o.flatten())\n\n    # End sentence of EOS\n    if eos == idx:\n      break\n\n    word = ''\n    if idx > 0:\n      word = idx2word_trans[idx]\n      output_sentence.append(word)\n\n    # Update the decoder input\n    # which is just the word just generated\n    target_seq[0, 0] = idx\n\n  return ' '.join(output_sentence)\n\n","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while True:\n  # Do some test translations\n  i = np.random.choice(len(input_texts))\n  input_seq = encoder_inputs[i:i+1]\n  translation = decode_sequence(input_seq)\n  print('-')\n  print('Input sentence:', input_texts[i])\n  print('Predicted translation:', translation)\n  print('Actual translation:', target_texts[i])\n\n  ans = input(\"Continue? [Y/n]\")\n  if ans and ans.lower().startswith('n'):\n    break\n","execution_count":null,"outputs":[{"output_type":"stream","text":"-\nInput sentence: Boil one egg.\nPredicted translation: kochen sie ein ei!\nActual translation: Kochen Sie ein Ei! <eos>\nContinue? [Y/n]Y\n-\nInput sentence: I hate this.\nPredicted translation: ich hasse die.\nActual translation: Ich hasse das. <eos>\nContinue? [Y/n]Y\n-\nInput sentence: We had fun.\nPredicted translation: wir hatten unseren spa.\nActual translation: Wir hatten unseren Spa. <eos>\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}